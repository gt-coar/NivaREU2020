{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VTraceTDLearner' object has no attribute 'norm_difference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-00189429046f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVTraceTDLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrho_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"T = {2**n}\\n Min Delta = {min(plot)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value Function: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValueFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-00189429046f>\u001b[0m in \u001b[0;36mrunEnvironment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploringStates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateValueFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_difference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_star\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VTraceTDLearner' object has no attribute 'norm_difference'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import mdptoolbox.example\n",
    "\n",
    "\n",
    "class VTraceTDLearner:\n",
    "    \"\"\"\n",
    "    Class that will ues on policy temporal difference\n",
    "    prediction to solve the model free problem with a given policy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_policy, behavior_policy, states, actions, alpha=0.1, gamma=0.95, beta = 0.5, diminish = None, T = 5, c_bar = 1, rho_bar = 1, iterations = 10000, epsilon = 0.01):\n",
    "        #action value function, mapping each state action pair\n",
    "        #initialized to have 0 for every new state action pair\n",
    "        self.value_function = np.zeros(states)\n",
    "\n",
    "        #discount rate between [0,1]\n",
    "        self.gamma = gamma\n",
    "\n",
    "        #step size between [0,1]\n",
    "        self.alpha = alpha\n",
    "\n",
    "        #the mapping of states to actions\n",
    "        self.target_policy = target_policy\n",
    "        self.behavior_policy = behavior_policy\n",
    "\n",
    "        #the amount of states and actions\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        #state space S\n",
    "        self.state_space = np.arange(0,self.states)\n",
    "        #action space A\n",
    "        self.action_space = np.arange(0,self.actions)\n",
    "\n",
    "        #transition probability matrix P, with shape AxSxS'\n",
    "        self.transitionProbabilities, self.rewardMatrix = mdptoolbox.example.rand(states,actions)\n",
    "        \n",
    "        #the reward matrix\n",
    "        self.rewardMatrix = np.random.rand(actions,states,states)\n",
    "\n",
    "        #transition probability matrix under policy pi\n",
    "        self.probabilityMatrix = np.zeros((self.states,self.states))\n",
    "        \n",
    "        \n",
    "        #using behavior policy to forming it into a matrix\n",
    "        self.policyMatrix = []\n",
    "        for a, p in self.behavior_policy.items():\n",
    "            self.policyMatrix.append(p)\n",
    "\n",
    "        \n",
    "        #only for diminishing step size using alpha and beta\n",
    "        self.diminish = diminish\n",
    "        self.step_size = self.alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        #amount of iterations to run the environment for\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        #keeps track of the amount of times the states have been visited\n",
    "        self.exploringStates = defaultdict(int)\n",
    "        \n",
    "        #estimated v_pi, using value iteration\n",
    "        self.v_pi = [10.52097262,10.48180422,10.9314057 ,10.71975961,10.75916237,10.66013344,10.79103673,10.63960651,10.46499304,10.48157663]\n",
    "        \n",
    "        #save the difference between v_pi and v\n",
    "        self.difference= []\n",
    "        \n",
    "        #n step trajecrtory you would like to \n",
    "        self.T = T\n",
    "        \n",
    "        #truncated weights\n",
    "        self.c_bar = c_bar\n",
    "        self.rho_bar = rho_bar\n",
    "        \n",
    "        #epsilon\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def setVpi(self,v_pi):\n",
    "        \"\"\"\n",
    "        Set the estimated v_pi to what you found\n",
    "        Must be called before running the environment\n",
    "        Default reward matrix is set to what was found in value iteration\n",
    "        \n",
    "        For this to work\n",
    "        :param v_pi: estimated v_pi, needs to be in length of state space S\n",
    "        :returns: void\n",
    "        \"\"\"\n",
    "        self.v_pi = v_pi  \n",
    "        \n",
    "    \n",
    "    def setRewardMatrix(self,matrix):\n",
    "        \"\"\"\n",
    "        Change the reward matrix to whatever you want with this function\n",
    "        Must be called before running the environment\n",
    "        Default reward matrix is set to random values between [0,1]\n",
    "        \n",
    "        For this to work\n",
    "        Matrix needs to be in shape AxSxS\n",
    "        A is the Action space\n",
    "        S is the State space\n",
    "        \n",
    "        :param matrix: the reward matrix of choice\n",
    "        :return: void\n",
    "        \"\"\"\n",
    "        self.rewardMatrix = matrix\n",
    "        \n",
    "    def createProbabilityMatrix(self):\n",
    "        \n",
    "        #to run TD, the transition probability matrix has to be under the policy \n",
    "        #taking the sum of the probabilities of taking that action a under state s and multiplying that by the transition probability value \n",
    "        \n",
    "        for s in range(self.states):\n",
    "            probabilities = np.zeros(self.states)\n",
    "            for a in range(self.actions):\n",
    "                    action_prob = self.behavior_policy[s][a]\n",
    "                    probabilities += self.transitionProbabilities[a][s]*action_prob\n",
    "            self.probabilityMatrix[s] = probabilities\n",
    "            \n",
    "\n",
    "    def getNextState(self, state, action):\n",
    "        \"\"\"\n",
    "        Just return the next state based on the transition probability matrix and the action taken\n",
    "        Using random choice that will use a distribution to give a state\n",
    "        :param state: observation that was previously seen\n",
    "        :param action: action that was take\n",
    "        :return: next state that the agent will go to \n",
    "        \"\"\"\n",
    "            \n",
    "        next_state = np.random.choice(self.state_space,1,p = self.probabilityMatrix[state])\n",
    "        return next_state[0]\n",
    "\n",
    "\n",
    "    def getValue(self,state):\n",
    "        \"\"\"\n",
    "        Just return the value of the state action pair given the current estimate of the environment\n",
    "        :param state: observation that was previously seen\n",
    "        :param action: action that was take\n",
    "        :return: the value of that state action pair\n",
    "        \"\"\"\n",
    "\n",
    "        return self.value_function[state]\n",
    "\n",
    "    def updateValueFunction(self,S0, S,A,R,timestep):\n",
    "        \"\"\"\n",
    "        Update the value function based on the\n",
    "        :param S0: initial state\n",
    "        :param S: state trajectory from time step 0 to time step T\n",
    "        :param A: action trajectory from time step 0 to time step T\n",
    "        :param R: reward trajectory from time step 0 to time step T\n",
    "        :return: void,will change the v_function in the class\n",
    "\n",
    "        using the off policy synchronous V trace temporal difference algorithm\n",
    "        V(s) = V(s) + ρt*β[R + βV(s+1) - V(s)]\n",
    "        \"\"\"\n",
    "        copy_v = self.value_function[S0]\n",
    "        \n",
    "        beta_summation = 0\n",
    "        for t in range(0,self.T+1):\n",
    "            c_j = 1\n",
    "            for j in range(0,t):\n",
    "                c_j*= min(self.c_bar, self.target_policy[S[j]][A[j]]/self.behavior_policy[S[j]][A[j]])\n",
    "                \n",
    "            rho_t = min(self.rho_bar, self.target_policy[S[t]][A[t]]/self.behavior_policy[S[t]][A[t]])\n",
    "            beta_summation  += (self.beta**t) * c_j*  rho_t *(R[t] + self.beta*self.value_function[S[t+1]] - self.value_function[S[t]])\n",
    "        self.value_function[S0] +=  self.epsilon* beta_summation\n",
    "        \n",
    "    \n",
    "    def chooseAction(self, state):\n",
    "        \"\"\"\n",
    "        Using the current state and the given policy find the action  using random choice based on the probabilities\n",
    "        \"\"\"\n",
    "        \n",
    "        p = []\n",
    "        for action,prob in sorted(self.behavior_policy[state].items()):\n",
    "            p.append(prob)\n",
    "            \n",
    "        action = np.random.choice(self.action_space, 1, p = p)\n",
    "        \n",
    "        return action[0] \n",
    "    \n",
    "    def takeAction(self, state, action):\n",
    "        \"\"\"\n",
    "        perform the action given to find the next position in the MDP\n",
    "        returns the next state and the reward from that transition\n",
    "        \"\"\"\n",
    "        next_state = self.getNextState(state,action)\n",
    "        reward = self.rewardMatrix[action][state][next_state]\n",
    "        \n",
    "        return next_state, reward\n",
    "    \n",
    "    \n",
    "    def runEnvironment(self):\n",
    "        \"\"\"\n",
    "        Will run the OpenGym AI environment set using on policy Temporal Difference Learning\n",
    "\n",
    "        :param episodes: the number of episodes you would like to make the enviornment play\n",
    "        :param maxtimesteps:\n",
    "        :return: void\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Rough Estimate of the Value Function Limit\n",
    "        [10.32698916 10.28997413 10.61215495 10.47182536 10.53804012 10.44741659\n",
    "         10.58984017 10.28781177 10.42280426 10.27326007]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        #create the transition probability matrix given the policy pi\n",
    "        self.createProbabilityMatrix()\n",
    "        \n",
    "        for t in range(self.iterations):\n",
    "\n",
    "            totalReward = 0\n",
    "            \n",
    "            #loop through the states\n",
    "            for s in range(0,self.states):\n",
    "                S = [s]\n",
    "                A = []\n",
    "                R = []\n",
    "                state = s\n",
    "                #create a T trajectory from state s\n",
    "                for t in range(0,self.T+1):\n",
    "                    action = self.chooseAction(state)\n",
    "                    next_state,reward = self.takeAction(state, action)\n",
    "                    S.append(next_state)\n",
    "                    A.append(action)\n",
    "                    R.append(reward)\n",
    "                    state= next_state\n",
    "                    self.exploringStates[next_state]+=1\n",
    "                #update the value function V(s)\n",
    "                self.updateValueFunction(s, S,A,R,t)\n",
    "                \n",
    "            #calculate the difference between v_pi and v\n",
    "            self.difference.append(np.linalg.norm(self.v_pi-self.value_function))\n",
    "            \n",
    "        return self.difference\n",
    "        \n",
    "    def plot(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Will plot the 2-norm difference found and more information on the results\n",
    "        For debugging purposes\n",
    "        \"\"\"\n",
    "        \n",
    "        #info on the MDP\n",
    "        print(f\"MDP with {self.states} states and {self.actions} actions\\ndiscounting factor gamma = {self.gamma}, step size alpha = {self.step_size}\\n\")\n",
    "        print(\"Running \", self.episodes, \"episodes\")\n",
    "        \n",
    "        #info on the policy\n",
    "        print(\"\\nThe Policy is (state,action) = target probability    behavior probability\")\n",
    "        for s in range(self.states):\n",
    "            for a in range(self.actions):\n",
    "                print(f\"({s},{a}) = {self.target_policy[s][a]}      {self.behavior_policy[s][a]} \",end=\" \")\n",
    "            print()\n",
    "                      \n",
    "        \n",
    "        #plot the difference between v-pi and v\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(range(len(self.difference)), self.difference, 'b-')\n",
    "        plt.title(\"V-trace TD with Importance Sampling\",fontsize = 20)\n",
    "        plt.xlabel(\"TimeStep\",fontsize = 20)\n",
    "        plt.ylabel(\"2-norm of Vπ - V\",fontsize = 20)\n",
    "        plt.xticks(fontsize = 14)\n",
    "        plt.yticks(fontsize = 14)\n",
    "        plt.show()\n",
    "        \n",
    "        #info on the value function\n",
    "        print(\"Value Function:  \", self.value_function)\n",
    "        print(\"\\nMinimum Delta:  \", min(self.norm_difference))\n",
    "    \n",
    "    \n",
    "    def getValueFunction(self):\n",
    "        return self.value_function\n",
    "\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    The Main Function to Run your program\n",
    "    Example code given below\n",
    "    All hyperparameters can be adjusted to your liking\n",
    "    \n",
    "    \n",
    "    Policy must be given  \n",
    "    the policy will be in the format of \n",
    "    (state,action) = Probability\n",
    "    \n",
    "    in python code it will be a dictionary inside of a dictionary\n",
    "    {state: {action: probability of that action, action: probabiity, ... } state:{...}}\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    target_policy = {0: {0: 0.4835164835164835, 1: 0.5164835164835165}, 1: {0: 0.48854961832061067, 1: 0.5114503816793893}, 2: {0: 0.881578947368421, 1: 0.11842105263157894}, 3: {0: 0.7980769230769231, 1: 0.20192307692307693}, 4: {0: 0.2926829268292683, 1: 0.7073170731707317}, 5: {0: 0.4430379746835443, 1: 0.5569620253164557}, 6: {0: 0.88, 1: 0.12}, 7: {0: 0.4715447154471545, 1: 0.5284552845528455}, 8: {0: 0.30952380952380953, 1: 0.6904761904761905}, 9: {0: 0.34328358208955223, 1: 0.6567164179104478}}\n",
    "    behavior_policy = {0: {0: 0.4351851851851852, 1: 0.5648148148148148}, 1: {0: 0.423728813559322, 1: 0.576271186440678}, 2: {0: 0.5714285714285714, 1: 0.42857142857142855}, 3: {0: 0.1, 1: 0.9}, 4: {0: 0.49382716049382713, 1: 0.5061728395061729}, 5: {0: 0.2824427480916031, 1: 0.7175572519083969}, 6: {0: 0.42168674698795183, 1: 0.5783132530120482}, 7: {0: 0.1, 1: 0.9}, 8: {0: 0.8636363636363636, 1: 0.13636363636363635}, 9: {0: 0.3401360544217687, 1: 0.6598639455782312}}\n",
    "    np.random.seed(0)\n",
    "    states = 10\n",
    "    actions = 2\n",
    "    c_bar = 1\n",
    "    rho_bar = 1\n",
    "    iterations = 10000\n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "    #multiple T values on the same plot\n",
    "    plt.figure(figsize = (15,5))\n",
    "    for n in range(0,7):\n",
    "        learner = VTraceTDLearner(beta = 0.95, T = 2**n,target_policy = target_policy, behavior_policy = target_policy,states = states, actions = actions, epsilon = 0.01, iterations = iterations, c_bar = c_bar, rho_bar = rho_bar)\n",
    "        plot = learner.runEnvironment()\n",
    "        print(f\"T = {2**n}\\n Min Delta = {min(plot)}\")\n",
    "        print(\"Value Function: \", learner.getValueFunction(),\"\\n\")\n",
    "        plt.plot(range(iterations), plot, label = f\"T = {2**n}\")\n",
    "    plt.legend(fontsize = 20)\n",
    "    plt.title(\"V-Trace Algorithm\", fontsize = 20)\n",
    "    plt.xlabel(\"Iteration\",fontsize = 20)\n",
    "    plt.ylabel(\"2-norm of Vπρ - V\",fontsize = 20)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #only one T value run\n",
    "    print(\"With Constant Step Size\\n\")\n",
    "    learner = VTraceTDLearner(target_policy = target_policy, behavior_policy = behavior_policy,states = states, actions = actions, epsilon = 0.01, iterations = iterations, c_bar = c_bar, rho_bar = rho_bar)\n",
    "    learner.runEnvironment()\n",
    "    learner.plot()\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\n\\nWith Diminishing Step Size\")\n",
    "    learner = TDLearner(policy=policy,states = states, actions = actions, diminish = lambda a,k,b: a/(k**b),iterations = 500000,alpha = 1)\n",
    "    learner.runEnvironment()\n",
    "    learner.plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
